[{"content":"TensorRT Converion from ONNX to TRT # Windows\rtrtexec.exe --onnx=INPUT_FILE --saveEngine=OUTPUT_FILE --fp16 --verbose\r# Linux\rtrtexec --onnx=INPUT_FILE --saveEngine=OUTPUT_FILE --fp16 --verbose Docker Mount a directory while initializing a container docker run -it -v DIRECTORY_TO_MOUNT:MOUNT_PATH --gpus all DOCKER_IMAGE ONNX Converion form PyTorch to ONNX import torch\r# model = DEFINE_MODEL\rmodel.eval() dummy_input = torch.randn(1, INPUT_SHAPE, requires_grad=True) torch.onnx.export(model, dummy_input, OUTPUT_FILE, export_params=True, opset_version=13, do_constant_folding=True, input_names = [\u0026#39;modelInput\u0026#39;], # the model\u0026#39;s input names\routput_names = [\u0026#39;modelOutput\u0026#39;], # the model\u0026#39;s output names dynamic_axes={\u0026#39;modelInput\u0026#39; : {0 : \u0026#39;batch_size\u0026#39;}, # variable length axes \u0026#39;modelOutput\u0026#39; : {0 : \u0026#39;batch_size\u0026#39;}}) Modify Input/Output Dimensions import onnx\rmodel = onnx.load(ONNX_INPUT_FILE)\rprint(model.graph.input)\rprint(model.graph.output)\r# change input dimensions as per model architecture\rinputs = model.graph.input\rfor input in inputs:\rinput.type.tensor_type.shape.dim[0].dim_value = 8 # BATCH SIZE\r# input.type.tensor_type.shape.dim[1].dim_value = 416\r# input.type.tensor_type.shape.dim[2].dim_value = 416\r# input.type.tensor_type.shape.dim[3].dim_value = 3\r# change output dimensions as per model architecture\routputs = model.graph.output\rfor output in outputs:\routput.type.tensor_type.shape.dim[0].dim_value = 8 # BATCH SIZE\r# output.type.tensor_type.shape.dim[1].dim_value = 7\ronnx.save(model, ONNX_OUTPUT_FILE) Fold constants polygraphy surgeon sanitize --fold-constants INPUT_FILE -o OUTPUT_FILE Python Make a requirements.txt for a project pip install pipreqs\rpipreqs Compile .py to binary .pyc # PYTHON\rimport argparse\rimport py_compile\rpy_compile.compile(INPUT_FILE, cfile=OUTPUT_FILE)\rConvert to executable pyinstaller -F FILE_NAME.py --hidden-import=FILE_NAME --hidden-import=OTHER_HIDDEN_IMPORTS FastAPI # For FILE_NAME.py containing application name as app\ruvicorn FILE_NAME:app --reload --port PORT --workers=NUM_WORKERS FFMPEG Video file commands # Loop a video ffmpeg -stream_loop -1 -t HH:MM:SS -i INPUT_FILE -c copy OUTPUT_FILE RTSP stream commands # Check if RTSP stream is working\rffprobe -v quiet -print_format json -timeout 5000000 -show_streams RTSP_LINK\r# Save stream in one big file\rffmpeg -i RTSP_LINK -nostats -fflags nobuffer -vcodec copy -reset_timestamps 1 -y FILE_NAME.EXTENSION\r# Save stream in segments of length SEGMENT_LENGTH seconds\rffmpeg -i RTSP_LINK -nostats -fflags nobuffer -vcodec copy -f segment -segment_time SEGMENT_LENGTH -reset_timestamps 1 -y FILE_NAME%d.EXTENSION\r# Create an RTSP stream from a video file\rffmpeg -re -i VIDEO_FILE_PATH -rtsp_transport tcp -c:v copy -an -f rtsp rtsp://localhost:PORT/OPTIONAL_URL_PATH Windows Install a service from an executable nssm.exe install SERVICE_NAME EXE_PATH ","permalink":"https://scholarstree.github.io/posts/2023-01-05-notes/2023-01-05-notes/","summary":"TensorRT Converion from ONNX to TRT # Windows\rtrtexec.exe --onnx=INPUT_FILE --saveEngine=OUTPUT_FILE --fp16 --verbose\r# Linux\rtrtexec --onnx=INPUT_FILE --saveEngine=OUTPUT_FILE --fp16 --verbose Docker Mount a directory while initializing a container docker run -it -v DIRECTORY_TO_MOUNT:MOUNT_PATH --gpus all DOCKER_IMAGE ONNX Converion form PyTorch to ONNX import torch\r# model = DEFINE_MODEL\rmodel.eval() dummy_input = torch.randn(1, INPUT_SHAPE, requires_grad=True) torch.onnx.export(model, dummy_input, OUTPUT_FILE, export_params=True, opset_version=13, do_constant_folding=True, input_names = [\u0026#39;modelInput\u0026#39;], # the model\u0026#39;s input names\routput_names = [\u0026#39;modelOutput\u0026#39;], # the model\u0026#39;s output names dynamic_axes={\u0026#39;modelInput\u0026#39; : {0 : \u0026#39;batch_size\u0026#39;}, # variable length axes \u0026#39;modelOutput\u0026#39; : {0 : \u0026#39;batch_size\u0026#39;}}) Modify Input/Output Dimensions import onnx\rmodel = onnx.","title":"How-To Notes"},{"content":" Notations Symbol Meaning $s_t$ state at timestep $t$ $a_t$ action taken in state $s_t$ $r(a_t \\vert s_t)$ reward after taking action $a_t$ in state $s_t$ $\\tau$ a trajectory, $(s_0, a_0, r_0, s_1, a_1, r_1, \\ldots, s_{T-1}, a_{T-1}, r_{T-1}, s_T)$ $r(\\tau)$ $\\sum_{t}r(a_{t}, s_{t})$ $\\pi_\\theta(\\tau)$ probability of tajectory $\\tau$ under policy $\\pi_\\theta$ with parameters $\\theta$ $\\gamma$ discount factor Theory Policy gradient methods try to solve reinforcement learning problems by modelling the policy as a parameterized function, generally a neural network. The policy network takes in observations as inputs and returns a distribution over available actions as output. An action from this distribution is sampled and a reward is received after the action is taken. The goal of policy gradient methods is to maximize (or minimize the negative of) an objective function which depends on the actions taken and rewards received over multiple timesteps. Our objective function, $J(Î¸)$ is the expectation of returns over all trajectories under policy $\\pi_\\theta$. We find the gradient of the objective functions w.r.t. the parameters of policy network and make updates.\n$$J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi {\\theta}(\\tau) \\bigg[ \\sum_tr(a_t, s_t) \\bigg] \\tag{1}\\label{eq:1}}$$\nwhere,\n$$ \\begin{align} \\pi_\\theta(\\tau) \u0026amp;= p(s_1)\\sum_{t=1}^{T} \\pi_\\theta(a_t \\vert s_t)p(s_{t+1} \\vert s_t, a_T) \\newline \\log \\pi_\\theta(\\tau) \u0026amp;= \\log p(s_1) + \\sum_{t=1}^{T} \\log\\pi_\\theta(a_t \\vert s_t) + \\log p(s_{t+1} \\vert s_t, a_T) \\end{align} $$\nThe gradient of $J(\\theta)$ has the following form.\n$$ = \\mathbb{E}{\\tau \\sim \\pi {\\theta}(\\tau)} \\bigg[ \\bigg( \\sum{t=1}^{T} \\nabla_\\theta\\log\\pi_\\theta(a_t \\vert s_t) \\bigg) \\sum_{t}r(a_{t}, s_{t}) \\bigg]\\tag{2}\\label{eq:2} $$\nComputing this expectation under a distribution over all trajectories is intractable, so we use sampling instead to make an approximation.\n$$\\nabla_\\theta J(\\theta) \\approx \\frac {1}{N} \\sum_{i=1}^{N} \\bigg[ \\bigg( \\sum_{t=1}^{T} \\log\\pi_\\theta(a_t \\vert s_t) \\bigg) \\sum_{t}r(a_{t}, s_{t}) \\bigg]\\tag{3}\\label{eq:3}$$\nA simple policy gradient algorithm repeats the following steps:\nSample $\\tau^{i}$ from $\\pi_\\theta(a_t \\vert s_t)$ Compute $\\nabla_\\theta J(\\theta) \\approx \\frac {1}{N} \\sum_{i} \\big( ( \\sum_{t} \\nabla_\\theta\\log\\pi_\\theta(a_t \\vert s_t) ) \\sum_{t=1}^{T}r(a_{it}, s_{it}) \\big)$ Update $\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta J(\\theta)$ This simple algorithm is prone to variance. While training policy gradients, there are certain methods used to reduce such variance. These methods usually make small changes in the objective function. In the following sections, the effects of these methods is demonstrated on CartPole-v0 environment.\nPractical consideration: Computing $\\nabla_\\theta\\log\\pi_\\theta(a_t \\vert s_t)$ explicitly is inefficient. We instead compute $$\\tilde{J}(\\theta) \\approx \\frac {1}{N} \\sum_{i=1}^{N} \\bigg[ \\bigg( \\sum_{t=1}^{T} \\log\\pi_\\theta(a_t \\vert s_t) \\bigg) \\sum_{t}r(a_{t}, s_{t}) \\bigg]\\tag{4}\\label{eq:4}$$ and use automatic differentiation to compute policy gradients.\nCartPole-v0 CartPole-v0 is an OpenAI gym environment where a pole is attached to a cart. The cart can move along a frictionless track. The goal is to prevent the pole from falling. CartPole-v0 observations are arrays of 4 values - cart position, cart velocity, pole angle and pole velocity at the top. The action is a discrete value - 0 (push cart to the left) or 1 (push cart to the right). The top limit on the number of steps that can occur in one episode is 200. There is another version, CartPole-v1 with 500 steps as max limit but experiments in this article will be based on CartPole-v0.\nExperiment details Policy network: inputs of dimensions (batch size, 4); 2 hidden layers of size 64 each; output dimensions of (batch size, 2); learning rate of 0.001 with Adam optimizer Baseline network: input of dimensions (batch size, 4); 2 hidden layers of size 64 each; output dimensions of (batch size, 1); learning rate of 0.001 with Adam optimizer Discount factor: 0.99 Random seeds: All experiments are performed thrice with numpy, pytorch and gym seeds as 0, 10 and 20. Results are averaged. Number of iterations: 1000. One iteration means using a batch to make one policy update. Plots: Average of returns in a batch vs iterations. Batch size Batch size refers to the number of timesteps used to make an update to policy network. When only a single timestep is used, the algorithm is called REINFORCE. Here, comparison is done between three batch sizes:\nAt least 1. Only a single episode is run. At least 500. Episodes are run until we get 500 timesteps. At least 1000. Episodes are run until we get 1000 timesteps. It is possible that we receive required minimum timesteps (1, 500 or 1000) in the middle of an episode but we still run the episode till the end and use all the timesteps from that episode in the batch. This leads us to having some extra timsteps in our batch. If we stop the episode in the middle, then the returns we get will be lower than actual returns. Now, let\u0026rsquo;s run our algorithm to maximize \\eqref{eq:4}.\nPolicy update using at least 1 timestep.\nWe can see that average returns of the batches are not consistent and have a lot of variance. Also, notice that the results are sensitive to random seeds as well. On increasing the batch size to 500, we get a considerable reduction in variance and also, a faster increase in average returns.\nPolicy update using at least 500 timesteps.\nThere are still inconsistencies around higher rewards. Using a batch size of 1000 gives us a smoother curve. These results were observed without using discounted returns. Next, we\u0026rsquo;ll change our objective function to use causality and discounted returns.\nPolicy update using at least 1000 timesteps.\nCausality and reward-to-go In the objective function used above, log probabilities of all taken actions are multiplied by sum of rewards. But policy at timestep $t\u0026rsquo;$ cannot affect reward at timestep $t$ when $t \u0026lt; t\u0026rsquo;$. So we modify the objective function to:\n$$\\tilde{J}(\\theta) \\approx \\frac{1}{N}\\sum_{i=1}^{N} \\bigg(\\sum_{t=1}^{T}\\log\\pi_\\theta(a_{it}\\vert s_{it})\\bigg)\\bigg(\\sum_{tâ=t}^{T} r(a_{itâ}\\vert s_{itâ})\\bigg)\\tag{5}\\label{eq:5}$$\nAlso, rewards received much later in future should have lesser contribution to returns than rewards received in near future. So, we use discounted returns to incorporate this dynamic behaviour.\n$$\\tilde{J}(\\theta) \\approx \\frac{1}{N}\\sum_{i=1}^{N} \\bigg(\\sum_{t=1}^{T}\\log\\pi_\\theta(a_{it}\\vert s_{it})\\bigg)\\bigg(\\sum_{tâ=t}^{T} \\gamma^{tâ-t} r(a_{itâ}\\vert s_{itâ})\\bigg)\\tag{6}\\label{eq:6}$$\nOn using causality and reward-to-go, our policy learns faster and reaches the maximum of average returns in fewer timesteps.\nReward-to-go (yellow) vs no reward-to-go (blue) using at least 1 timestep.\nReward-to-go (yellow) vs no reward-to-go (blue) using at least 500 timestep.\nReward-to-go (yellow) vs no reward-to-go (blue) using at least 1000 timestep.\nBaselines A popular variation of the objective function to reduce variance of gradient function is to subtract a baseline from the returns. A common baseline to subtract is state-value function and the resulting value is called advantage.\n$$\\tilde{J}(\\theta) \\approx \\frac {1}{N} \\sum_{i=1}^{N} \\bigg[ \\bigg( \\sum_{t=1}^{T} \\log\\pi_\\theta(a_t \\vert s_t) \\bigg) \\bigg(\\sum_{t\u0026rsquo;=t}^{T}r(a_{it}, s_{it}) - b\\bigg) \\bigg]\\tag{7}\\label{eq:7}$$\nWe now optimize \\eqref{eq:7} using a neural network baseline\nBaseline (yellow) vs no baseline (blue) using at least 1 timestep.\nBaseline (yellow) vs no baseline (blue) using at least 500 timestep.\nBaseline (yellow) vs no baseline (blue) using at least 1000 timestep.\nTraining plots show that baseline method works better for large batch sizes. It does not seem to work with small batch sizes (or when our batch consists of only one episosde).\nAdvantage normalization Another trick to make policy gradients work is normalizing returns or advantage (in case of baselines). Following are the results of normalizing returns, baselines were not used in this case.\nNormalization (yellow) vs no normalization (blue) using at least 1 timestep.\nNormalization (yellow) vs no normalization (blue) using at least 500 timestep.\nNormalization (yellow) vs no normalization (blue) using at least 1000 timestep.\nFinal result This time we use everything we have in our toolset - causality, reward-to-go, baselines and advantage normalization.\nUsing causality, reward-to-go, baselines and advantage normalization.\nYou can see that our small batch size is performing poorly due to added baseline.\nIs there a fix to make baseline work with smaller batches? So far, we have used the objective function of the form\n$$\\tilde{J}(\\theta) \\approx \\frac {1}{N} \\sum_{i=1}^{N} \\big[ \\big( \\sum_{t=1}^{T} \\log\\pi_\\theta(a_t \\vert s_t) \\big) \\sum_{t}r(a_{t}, s_{t}) \\big]$$\nwhich can be thought of as an average of returns over sampled trajectories. We can instead use average of returns over all timesteps i.e.\n$$\\tilde{J}(\\theta) \\approx \\frac {1}{\\tilde{T}} \\sum_{i=1}^{N} \\bigg[ \\bigg( \\sum_{t=1}^{T} \\log\\pi_\\theta(a_t \\vert s_t) \\bigg) \\sum_{t}r(a_{t}, s_{t}) \\bigg]\\tag{8}\\label{eq:8}$$\nwhere $\\tilde{T}$ is total timesteps in a batch. Optimizing \\eqref{eq:8} fixes the baseline inconsistency for small batches.\nUsing average of returns over all timesteps.\nReferences Berkeley Deep RL Course Policy Gradient Algorithms Going Deeper Into Reinforcement Learning: Fundamentals of Policy Gradients ","permalink":"https://scholarstree.github.io/posts/2019-06-05-training-policy-gradients/2019-06-05-training-policy-gradients/","summary":"Notations Symbol Meaning $s_t$ state at timestep $t$ $a_t$ action taken in state $s_t$ $r(a_t \\vert s_t)$ reward after taking action $a_t$ in state $s_t$ $\\tau$ a trajectory, $(s_0, a_0, r_0, s_1, a_1, r_1, \\ldots, s_{T-1}, a_{T-1}, r_{T-1}, s_T)$ $r(\\tau)$ $\\sum_{t}r(a_{t}, s_{t})$ $\\pi_\\theta(\\tau)$ probability of tajectory $\\tau$ under policy $\\pi_\\theta$ with parameters $\\theta$ $\\gamma$ discount factor Theory Policy gradient methods try to solve reinforcement learning problems by modelling the policy as a parameterized function, generally a neural network.","title":"Training Policy Gradients"},{"content":" I had recently started reading An Era of Darkness: The British Empire in India and was dazzled by Dr. Tharoor\u0026rsquo;s wizardry of words. He calls himself an amateur historian while painting an honest picture of British Raj in India. I was mesmerized by his writing and wondered if any underlying pattern could be found using Deep Learning. Hackfest, our annual college hackathon was also approaching around the time, so I decided to execute a project related to Dr. Tharoor\u0026rsquo;s literature in 36 hours. So when the contest started, we were trying to build a Tharoorian text classifier.\nWe planned to make a lightweight web application capable of taking a piece of text as input and estimating the probability of it being written by Dr. Tharoor. We were aware that it would be impossible to make a deployable model in just 36 hours. Nevertheless, we worked out a planâ-âGather and process data, train a model, deploy on a local server and keep improving the model.\nDataset We collected 10 of Dr. Tharoor\u0026rsquo;s published books, extracted ~30,000 sentences and labeled them as 1 while manually removing short sentences consisting of only a few words. We randomly picked some books for negative samples and labeled ~200,000 sentences as 0. 100d GloVe embeddings were used for creating the embedding matrix - vector representation of words in the dataset.\nSentence Label The British Raj is scarcely ancient history. 1 The reason was simple: India was governed for the benefit of Britain. 1 By breaking the law non-violently he showed up the injustice of the law. 1 This is not my fault. 0 Grown-ups never understand anything by themselves, and it is exhausting for children to have to explain over and over again. 0 Model We decided to use a Bidirectional LSTM for our purpose. Although we also tried 1D Convolutions which produced funny results. Training a bi-LSTM netowrk took 1 hour on our GPU. After our Keras model was trained, we exported it into aÂ .h5 file and used a flask server to deploy it to a local server.\nResults Although our model was a simple LSTM network without any attention, it gave decent predictions. Since the input to our network were single sentences only, it mostly depended upon presence of words to make predictions. Our training dataset only consisted of his books which kind of restricted our model\u0026rsquo;s ability. Perhaps adding more data and using attention models would have given better results.\nSentence Prediction(%) Earth is flat. 49.42 I will build a great, great wall on our southern border, and I will have Mexico pay for that wall. Mark my words. 84.38 There was an idea to bring together a group of remarkable people, so when we needed them, they could fight the battles that we never could. 96.64 Exasperating farrago of distortions, misrepresentations \u0026amp; outright lies being broadcast by an unprincipled showman masquerading as a journalist. 87.03 Albus Dumbledore didn\u0026rsquo;t seem to realize that he had just arrived in a street where everything from his name to his boots was unwelcome. 0.05 The sheep are running wild. 2.83 History is boring. 90.49 Physics is boring. 37.65 I is extremely tired of reading history. 22.47 Conclusion Our team secured 6th rank out of ~50 participants. Along with this project, we also attempted Samsung\u0026rsquo;s Bixby challenge based on NLP and were 2nd runner-up.\n","permalink":"https://scholarstree.github.io/posts/2018-04-27-tharoor-it/2018-04-27-tharoor-it/","summary":"I had recently started reading An Era of Darkness: The British Empire in India and was dazzled by Dr. Tharoor\u0026rsquo;s wizardry of words. He calls himself an amateur historian while painting an honest picture of British Raj in India. I was mesmerized by his writing and wondered if any underlying pattern could be found using Deep Learning. Hackfest, our annual college hackathon was also approaching around the time, so I decided to execute a project related to Dr.","title":"Tharoor It"},{"content":" Quadcopter Components\nI\u0026rsquo;ve spent my last six months assembling, breaking, fixing, breaking and reassembling a quadcopter. Before I started, I had assumed that the task would be easy but I had no idea about the effort it would take to make a copter fly. In this post, I share my experience which would probably save others from repeating my mistakes.\nPlanning Before purchasing the required components, it would be better to make an outline of the procedure to be followed. In my medium blog post, I listed a few problems to consider before getting started. It is a time consuming task. I spent a lot of time comparing the components before purchasing, reading documentations and watching youtube videos. I considered this as my side project and it soon turned out to be demanding.\nNeedless to say, this would be an expensive project requiring contuinuous purchase of broken parts. Aproaching college administration might help; my project is funded by my project mentor but it took some time to secure it.\nAs a beginner, it would be difficult to do all the tasks by oneself. I did not pay importance to the idea of adding more members to the team and it was probably the biggest mistake I made. Increasing team size is my current priority.\nAlso, the goal of the project should be figured it out as early as possible and relative research should be started. We started out with a purpose of drone delivery and aerial photography, though I don\u0026rsquo;t see it completed anytime soon. There needs to be a mental preparation for facing a lot of problems. There\u0026rsquo;d be crashes, software bugs and innumerable incomprehensible hard-to-find-solutions-to problems. Sometimes, days would be spent trying to find a solution to a problem which would later turn out to be as simple as firmware update.\nPurchasing components There are a few components that are essential for a quadcopter, rest are purpose dependent. I found Flying Car Nanodegree\u0026rsquo;s first lesson a good source for introduction. What\u0026rsquo;s necessary for a quadcopter is a frame, four motors, four ESCs, a flight controller, a transmitter, a receiver, a battery and a few sensors (an accelometer, a gyroscope and a compass). Optionally, one can add a GPS module, a companion computer, a camera, etc.\nOur copter currently has following components:\n1 x Frame 4 x Motor 4 x ESC 4 x Propeller 1 x Transmitter+Receiver 1 x Pixhawk 1 x Telemetry Module 1 x Lipo battery Understanding the components I had to watch a lot of youtube videos to understand the need and the order in which the components should be assembled. This is the reason one should start early; there\u0026rsquo;s going to be a lot of research before you can begin assembling your copter.\nThe frame holds all the components together. The copter has to be powered using a battery which would have direct connections to 4 motors (via ESCs) and a flight controller. The flight controller would manage power distribution to all other components - the transmitter, GPS module, onboard computer etc. Our frame has in-built power distribution connections. We soldered 4 ESCs to the board to attach them permanently. ESCs (Electronic Speed Controllers) are used to vary motor\u0026rsquo;s speed and direction of rotation. Therefore, an ESC is needed for every motor.\nA motor is fixed on each leg of the copter and connected with their respective ESCs. It is important to not attach propellers till the end for safety purpose. Controlling the speed and direction of every motor manually would be cumbersome and inefficient. Therefore, a flight controller is used for the purpose; it has connection with all ESCs.\nA receiver is used to give commands to the copter from ground and a transmitter attached on the copter would receive those commands and send it to flight controller. The flight controller would then send appropriate commands to ESCs which in turn rotate the motors accordingly.\nA simple command would follow the following route:\\ Transmitter \u0026ndash;\u0026gt; Receiver \u0026ndash;\u0026gt; Flight Controller \u0026ndash;\u0026gt; ESCs \u0026ndash;\u0026gt;Motors\nNow, the frame has 4 motors, 4 ESCs, a flight controller, a transmitter, a receiver and a battery. We used Pixhawk as flight conmtroller which has inbuilt an accelerometer, a compass and a gyroscope, all of which have to be calibrated.\nCalibration All the sensors need to be calibrated to give flight controller the information about frame and the components. Calibration can be easily done using Mission Planner which gives instructions for every step. You\u0026rsquo;d need to connect to a computer via USB cable though. If you don\u0026rsquo;t want to connect via USB you can use radio telemetry which allows you to connect Mission Planner to your copter using radio signals. Telemetry would consist of a receiver (connected to your computer via USB) and a transmitter (connected to fight controller on the copter). Connecting to your copter via telemetry will allow you to analyze various parameters like speed, altitude, remaining, battery in Mission Planner.\nWe also need to bind our radio transmitter to receiver, calibrate ESCs and arm our copter before flying. All of these calibration methods will vary a little depending upon the kind of flight controller and other components you have. Reading documentation is the easiest way to figure out the steps required.\nCompanion computer For onboard processing, a companion computer is added to the copter which can Raspbery Pi, Intel Edison, Nvidia TX, etc. We connected Raspberry Pi with pixhawk using mavproxy and dronekit-python. Dronekit-python allows to make and deploy apps on the copter.\nFlying bugs Every time you want to fly, you\u0026rsquo;d have to arm the copter which means checking all the components and starting power supply to the motors. Then, you\u0026rsquo;re ready to use sticks in your transmitter to play with your copter. It sounds easy, but when we tried it out, it took us a few days to get it stable in the air. Following are the mistakes we made:\nWrong motor spinning direction A quadcopter can have various frame types and each frame type would have different motor rotation configuration. In out X configutaion, diagonally opposite motors were supposed to be spinning in the same direction.\nUsing wrong propellers There are two kinds of propellers - clockwise spinning and anticlockwise spinning and they go with their respecting motors.\nPoor parameter tuning PID tuning is one of the most important step in stablizing a copter. We had been ignoring this step and it resulted in random crashes.\n","permalink":"https://scholarstree.github.io/posts/2018-03-10-beginners-guide-to-assembling-a-drone/2018-03-10-beginners-guide-to-assembling-a-drone/","summary":"Quadcopter Components\nI\u0026rsquo;ve spent my last six months assembling, breaking, fixing, breaking and reassembling a quadcopter. Before I started, I had assumed that the task would be easy but I had no idea about the effort it would take to make a copter fly. In this post, I share my experience which would probably save others from repeating my mistakes.\nPlanning Before purchasing the required components, it would be better to make an outline of the procedure to be followed.","title":"A Student's Guide to Assembling a Quadcopter"},{"content":" A year ago, I read Andrej\u0026rsquo;s blog post where he analyzed selfies using Convolutional Neural Networks. I was intrigued by the visualization technique he used which grouped images in such a way that nearby images were similar. In this post, I visualised my own collection of images in a square grid.\nSteps Involved t-Distributed Stochastic Neighbor Embedding (t-SNE) is a technique for visualizing high dimensional data in 2D or 3D. It defines a cost function between a joint probability distribution,P, in the high-dimensional space and a joint probability distribution, Q, in the low-dimensional space and minimizes that cost function. In this case, we\u0026rsquo;re plotting images but it can be used with all other kinds of data.\nt-SNE visualization of MNIST data.\nt-SNE visualization of MNIST data in a grid. (Not related to the previous visualization).\nThe full script can be found here. The script requires keras and tensorflow. It is tested with tensorflow (1.4.0) and keras (2.1.1).\nBuild model: VGG16 network (without top fc layers) is used and outputs of last conv block are flattened to form a vector.\nLoad images: All the images from the source directory are loaded.\nGenerate high dimensional representations: A single forward pass through the network generates the high dimensional representations.\nGet 2D point locations: t-SNE implementation of scikit-learn converts these representations to 2D data points.\nDistribute 2D representations in a square grid and save images: Finally, jonker-volgenant algorithm distributes these 2D points into a square grid and we assign every point in the grid a small image.\nt-SNE visualization of some random images in a grid.\nSystem Details HP Pavilion\n4 GB RAM\nNvidia GeForce GT 740M\nReferences L.J.P. van der Maaten and G.E. Hinton. Visualizing High-Dimensional Data Using t-SNE. Journal of Machine Learning Research 9(Nov):2579â2605, 2008.[PDF][Supplemental material][Talk][Code] ","permalink":"https://scholarstree.github.io/posts/2018-02-03-tsne-visualization-of-instagram-posts/2018-02-03-tsne-visualization-of-instagram-posts/","summary":"A year ago, I read Andrej\u0026rsquo;s blog post where he analyzed selfies using Convolutional Neural Networks. I was intrigued by the visualization technique he used which grouped images in such a way that nearby images were similar. In this post, I visualised my own collection of images in a square grid.\nSteps Involved t-Distributed Stochastic Neighbor Embedding (t-SNE) is a technique for visualizing high dimensional data in 2D or 3D.","title":"t-SNE Visualization of Instagram Posts"}]